#+TITLE: A Primer

This is largely an aid to my own memory so bear with me.

* The Syntax
  I'll start by describing the syntax. Not because it's the most important nor
  the best place to start, but because it's the first thing you see.

  The syntax is, for the most part, inspired by Clojure. In particular, the
  literal data structures: =[]=, ={}=, & =#{}= mean what you expect.

  One major goal is that if an expression *looks* familiar, then it should act in
  a familiar way. The degree to which I've met that goal is an open question,
  but things are still in flux.

  #+BEGIN_SRC clojure
    (def f (fn [x y] (+ x y)))
    ;; => f

    (f 4 5)
    ;; => 9

    (if (< (f x y) 0) (g x) (h y))
  #+END_SRC

  and so on.
** Pairs vs Cons Cells
   I wanted to avoid cons cells entirely in this language, but I'm starting to
   realise why they're important: the symmetry between calling a function as (f
   x y z) and defining it as (fn [x y z] ...). The fact that the tail of the
   cons cell (f x y z) is the cons cell (x y z) makes this automatic. So... if
   I want to be able to call things with a single arg that isn't necessarily a
   list, I need improper cons cells. Once we have that, the convention of
   multiple args becomes automatic.

   That said, there are no cons *lists*. The linked list was once a great idea,
   but is, on modern hardware, just about the worst possible way to store
   data.

   Lisp uses cons lists in two ways: 1) as lists of data, for which we'll just
   use vectors, and 2) as a syntax for invoking functions.

   (f x y z) means eval the head (f), eval the tail (x y z), then apply the
   latter to the former.

   We will keep the (f x y z) syntax for calling functions, but instead of
   (cons f (cons x (cons y (cons z nil)))), we store it as (pair f [x y z]).

   This (f x y z) is identical to (f . [x y z]).

   The =.= can be interpreted in a functional sense as =apply=, or in an actor
   sense as =send=. It turns out they're both equivalent with the right
   underlying abstractions.
** No Implicit Evaluation
   In constrast to most lisps (in fact all I've ever found save one called
   [[https://web.cs.wpi.edu/~jshutt/kernel.html][Kernel]]), we have no implicit evaluation. That means that both arguments and
   return values are passed by name. That sounds crazy. If not for the Kernel
   thesis I'd never have believed such a scheme could work, but it does!

   To explain, I'll need to jump ahead to μ.
** The μ Operator
   Why μ? Two reasons.

   1) it's the next letter after λ.
   2) It looks weird, and I want things that don't act as expected to look
      abnormal.

   μ defines pure syntax operations.

   Example:

   #+BEGIN_SRC clojure
     (def f (μ [x y] (+ x y)))
     ;; => f

     (f 4 5)
     ;; => (+ x y)

     ;; which is an error because x and y aren't defined.
   #+END_SRC

   μ doesn't define a macro. A μ is first class: they can be passed and returned
   as values, they operate at runtime (or compile time, or anytime).

   But a μ has no direct access to the environment (nor the continuations). You
   can pass a namespace to a μ (it's just a map) and take it apart so as to
   perform any reflective feat your heart may desire, but the lexical environment
   of an expression is immutable and the expression must be redefined to change
   it, so there's nothing that can be done by the body of a μ with its own
   lexical environment.

   Also remember that macros evaluate their own output (or properly speaking the
   compiler expands the macro and then calls =eval= on the result), whereas μs do
   not.

   But if nothing evaluates the output of a μ, how do they *do* anything?
** Immediate Evaluation
   The answer to the hanging question of the last section is that there is a
   syntactic expression that "means" =eval=.

   In reality it means "evaluate the following expression as soon as you have
   all of the information required to do so", which will remain ambiguous until
   we get into the details of the AST.

   That syntax is the tilde =~=.

   #+BEGIN_SRC clojure
     (def f (μ [x y] (+ ~x ~y)))
     ;; => f

     (f 4 5)
     ;; => (+ 4 5)

     ;; `~x` and `~y` can be evaluated as soon as `f` is called. Since they're
     ;; bound in the lexical environment of the body, eval reduces to lookup.
     ;; But remember that the output is not evaluated, so `f` returns an expression.

     (def g (μ [x y] ~(+ x y)))
     ;; => g

     (g 4 5)
     ;; => 9

     ;; Note that this only works because `+` is defined so as to explicitly eval
     ;; its arguments before applying the underlying addition operation.
   #+END_SRC

   Given a function that only works on literal values (say a version of =+=
   called =+*=), we can define an operator =wrap= which will invoke it in the
   standard applicative manner by first evaluating its arguments:

   #+BEGIN_SRC clojure
     (def wrap
       (μ f
          ~(μ args
              ~(~f . ~~args))))

     (def + (wrap . +*))
   #+END_SRC

   We can even define λ (here called =fn=) as a μ which receives arguments,
   evaluates them, passes the evaluated values to another μ and then evaluates
   its output:

   #+BEGIN_SRC clojure
     (def fn
       (μ [params body]
          ~(μ args
              ~((μ ~params ~~body) . ~~args))))
   #+END_SRC

   This is the actual definition of =fn= at the core of the language.

   It may help to think of the body of a μ as quasiquoted, except that we can
   unquote as many times as we please.

   That said, there is no quote, unquote, quasiquote, eval, nor apply defined in
   the language — you can easily write them yourself, but please don't — so the
   analogy is a bit weak.
** An Exception
   One final note: I said above that there is *no* implicit evaluation. You've
   probably noticed in my examples that everything typed into the repl *is*
   automatically evaluated. The same holds for the code reader =loadfile=. This
   is a convenience that once again makes code that acts as expected look
   normal. Also there's no point entering code into a repl unless you want it to
   be evaluated, so it seems like the right thing to do.

   μs are, of course, free to evaluate their arguments when and as they see fit.
   =def= adds an evaluation to the beginning of its body, for instance. I think
   that's the right thing to do as well, but it adds a bit of asymmetry to the
   definition of metaprogramming operations like =fn= and =wrap=.
* Outline
  Sections to be written.
** History, Environment, and Context
** Purity and the State (statefuls)
** Transduction
** Message Passing
** JIT and runtime optimisation
   The AST itself is optimisable, large parts can be evaluated immediately, and
   it's fairly fast to interpret at runtime.

   I also want a native compiler. The up side of strict immutability, reentrant
   code, and static reference is that we never have to compile a form more than
   once. The compiled code will always be the same if we compile it the same
   way.

   But that said, we do want to try and compile things in different ways
   depending on profiling during real usage and compare performance (say compile
   both to cpu and gpu and instrument to figure out how much data makes the
   switch worthwhile). But that's just a cache of code plus compiler options.

   My intuition tells me that if we get the static reference right and most code
   never changes (only change code if there's a security vulnerability, or a
   massive and much needed performance improvement) then over time more and more
   code will be compiled in more and more ways so that very little will be
   interpreted and most of the runtime's work will be in applying heuristics to
   choose which compiled code to run on which occasion.

   Hardly a trivial problem, but there's real opportunity here.
** Work stealing
** Offloading work to GPU
   Most consumer machines have gpus nowadays and a lot of streaming tasks map
   natuarally to them — though many others don't.

   Figuring out which can only be done in general at runtime.
** Compiler
** FFI / Distributed Execution
   These aren't normally the same thing, but they both boil down to talking to
   other computers, so their solutions largely overlap.
* Comparisons
** FP
   Immutability? Check. Big check.

   Functions? Not really. Mathematical functions can be implemented easily, but
   they're a special case of a more general construct.
** FBP
   I had never heard of FBP until quite recently. Reading Morrison's book (2010)
   makes it clear that a lot of my ideas are not new and have been in use —
   albeit obscurely — for decades.

   Particularly the delivery guarantees of which I was pretty proud. But at
   least I know now that what seemed obviously sufficient has been battle tested
   and does seem to suffice.

   These aren't bad things. I'm not terribly interested in priority. I'm
   interested in making this work and the validation is appreciated.

   That said, there are some important differences between my goals and
   Morrison's description. A lot of those differences come from ideas that are
   relatively new. The idea that immutability can work at scale is not widely
   accepted even today. I doubt it was even conceivable thirty years ago.

   But let's list them out:
*** memory management
    In FBP they take an ownership approach where datagrams must be manually
    allocated and freed. This stems from the fact that each node in the network
    bangs on the memory passed to it.

    I take a different approach in that all messages are immutable. They don't
    need manual management since reference counting suffices. They also don't
    need to be owned, so one unit can send the same message to a dozen others.
    There are other advantages as well.

    The cost of immutability amortises away with isolated mutability.
*** Graphicality
    I'm a big fan of graphical programming, but I don't think it should be
    primary. The primary needs to be machine readable. Machine readability is
    the only way to get widespread interoperability. Short of Lanier's "use ai
    to click buttons as an API" which is very intriguing, but I'm still dubious.

    I contend that being more machine friendly at a low level allows you to be
    more human friendly at a high level. This is mostly because the majority of
    programming language bullshit is about making it easier to for the
    developers of the compiler/editor/whatever, and if their job is already
    easy, they have fewer excuses.

    So instead I'm focusing on a traditional language with a (mostly)
    traditional AST, interpreter, and compiler. From that AST we can define
    isomorphisms with both the textual source code (or even multiple textual
    source codes) and a graphical representation.

    So draw your program at a high level and then go in later and tweak the
    code. Or vice versa. Or just stick to whichever you prefer.

    Graphical languages always have a layer of warts between the rubber and the
    road and I don't want that.
*** Fractal Structure
    FBP has an explicitly fractal structure, so if you see a graph of
    components, you can zoom in on any piece and see a nested graph of
    components. This can go on for an indefinite period of time before finally
    bottoming out on something atomic.

    I think there's something fundamentally right about that structure.

    I'm not sure how to encourage it in the current iteration of this language.

    Any set of functions corresponds to fbp would call the input ports of a
    network. The rest of the network is implicit in the call graph, but that's
    easy enough to follow along and layout on demand.

    So a network could be represented by a map of named functions.

    One major problem is that channels (ports) are not static. They can be
    dynamically generated. It can be, in general, impossible to determine the
    set of possible outputs of such a network statically since it may generate
    functions at runtime and plug them into the network. Technically those
    dynamic plugins would be subnetworks, but I don't know that that makes a
    difference.

    That dynamism isn't something I'm considering stripping from the language.
    That said, most of the time emissions go to named channels and the names are
    easily inferred from the code. There's also the secondary check that
    messages can only be sent on channels in the current context or newly
    generated ones, which gives us some foreknowledge. Likely not enough.
*** Holarchy
    The idea that a network is a whole unto itself in the sense that given
    inputs it does its thing and can't be intereferred with, but simultaneously
    subordinate in the sense that it doesn't control — and doesn't know —
    whither comes its input or whence goes its output, so that its place in the
    larger program is ordained from above, is implicit in what I've read of FBP,
    but never stated as such. Just taken for granted.

    I think — but don't know — that making this explicit by encouraging every
    subnetwork to believe itself to be the entire system will greatly
    fascilitate the composition of systems.

    Somewhat hyperbolically, I want a language whose todomvc is kubernetes.
** Structured Programming
   Here's a fun one: is this a structured language?

   Well, the fundamental unit of control is longjmp, so that might be a no.

   Arbitrary gotos aren't possible; there's still quite a bit of structure. It's
   more like an actor system or coroutines where you can send control to any
   agent you know how to reach.

   Loops and branches are accomplished by selecting the continuation of the
   current computation.

   This is a silly digression.
